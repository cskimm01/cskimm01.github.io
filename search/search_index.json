{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home: Chris Kimmons","text":""},{"location":"#welcome-to-chris-kimmons-documentation-site","title":"Welcome to Chris Kimmons' Documentation Site!","text":""},{"location":"#about-me","title":"About Me","text":"<p>Hi, I'm Chris!  I am data professional with a passion for problem solving and studying our world.  I had been a musician and a teacher for most of my career, and in 2022 I decided to pivot to data engineering/analytics.  My projects so far have included studying COVID-19 epidemiology, randomness in state lottery drawings, visualizing melodic material in music, and scraping weekly stock reports.</p> <p>Outside of data, I have a wonderful wife and two boys.  We like to play music together, walk in the woods, and go to museums.</p>"},{"location":"#please-feel-free-to-reach-out","title":"Please feel free to reach out!","text":"<p>cskimm02@gmail.com</p> <p></p>"},{"location":"covid_blog/","title":"Project 1: NYT Covid Dashboard","text":"<p>This project focuses on database architecture fundamentals, Tableau dashboard building, and data cleaning</p> In\u00a0[2]: Copied! <pre>import requests\nimport pandas as pd\nimport numpy as np\nimport os\nimport glob\nimport datetime as dt\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\n</pre> import requests import pandas as pd import numpy as np import os import glob import datetime as dt from matplotlib import pyplot as plt import seaborn as sns from IPython.display import Image <p>The US total sheet, US states sheet and county level data were separated for simplicity.  Also, the data grew unwieldy at the county level, so the NYT data professionals separated county data by year.  The US total and State totals were easy enough, but the counties needed to be concatenated to show trends over the three years.</p> In\u00a0[2]: Copied! <pre>us_df = (pd.read_csv('/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us.csv')\n         .astype({'date':'datetime64'}))\nus_df.info()\n</pre> us_df = (pd.read_csv('/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us.csv')          .astype({'date':'datetime64'})) us_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1158 entries, 0 to 1157\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   date    1158 non-null   datetime64[ns]\n 1   cases   1158 non-null   int64         \n 2   deaths  1158 non-null   int64         \ndtypes: datetime64[ns](1), int64(2)\nmemory usage: 27.3 KB\n</pre> In\u00a0[3]: Copied! <pre>state_df = (pd.read_csv('/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us-states.csv')\n            .astype({'date':'datetime64',\n                     'state':'category'})\n                     )\nstate_df.info()\n</pre> state_df = (pd.read_csv('/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us-states.csv')             .astype({'date':'datetime64',                      'state':'category'})                      ) state_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 61942 entries, 0 to 61941\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   date    61942 non-null  datetime64[ns]\n 1   state   61942 non-null  category      \n 2   fips    61942 non-null  int64         \n 3   cases   61942 non-null  int64         \n 4   deaths  61942 non-null  int64         \ndtypes: category(1), datetime64[ns](1), int64(3)\nmemory usage: 2.0 MB\n</pre> In\u00a0[4]: Copied! <pre># this is only one file of four used for county-level analysis\ncounty_df = pd.read_csv('/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us-counties-2023.csv')\ncounty_df.info()\n</pre> # this is only one file of four used for county-level analysis county_df = pd.read_csv('/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us-counties-2023.csv') county_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 267009 entries, 0 to 267008\nData columns (total 6 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   date    267009 non-null  object \n 1   county  267009 non-null  object \n 2   state   267009 non-null  object \n 3   fips    264023 non-null  float64\n 4   cases   267009 non-null  int64  \n 5   deaths  260613 non-null  float64\ndtypes: float64(2), int64(1), object(3)\nmemory usage: 12.2+ MB\n</pre> <p>And while all of this data is great, I had no data on populations.  I tried a few approaches, including a very clumsy webscraping approach to scrape population data from Wikipedia (using a combination of URLlib and Beautiful Soup) when I then realized I could just download population data from the US Census Bureau as a CSV.  But the first file i found was only for states, no territories.  So then I had to go dig through the US Census Site to find population data for Puerto Rico, American Samoa, the Marianas Islands, Guam, and the US Virgin Islands.  And I found it, even though the figures for the territories were population estimates, whereas the figures for the 50 states were the census base totals from 2020.</p> <p>Then there were some missing fips numbers for the territories, so I scraped the fips numbers from wikipedia into a csv.  And for good measure, I scraped the Abbreviations for each state and territory into a csv.  At the end the file for my data sources looked like this...</p> In\u00a0[5]: Copied! <pre>directory = '/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/'\n\ncovid_case_files = list(glob.glob(os.path.join(directory,'*.*')))\ncovid_case_files.sort()\n\nprint(*covid_case_files, sep='\\n')\n</pre> directory = '/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/'  covid_case_files = list(glob.glob(os.path.join(directory,'*.*'))) covid_case_files.sort()  print(*covid_case_files, sep='\\n') <pre>/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/PRM-EST2020.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/US_fips_codes.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/american-samoa-phc-table01.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/co-est2021-alldata.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/commonwealth-northern-mariana-islands-phc-table01.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/guam-phc-table01.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/island_populations.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/state_abbr.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us-counties-2020.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us-counties-2021.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us-counties-2022.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us-counties-2023.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us-states.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us-virgin-islands-phc-table01.csv\n/Users/chriskimmons/My_Projects/NYT_COV/covidCSV/us.csv\n</pre> <p>My mentor recommended that I shouldn't worry about the protectorates, since they likely weren't central to the data.  I went ahead with them because I wanted to know that I could, but in the end he was right. There was a lot of unusable data lying around that needed to be cleaned up to help with efficiency.</p> In\u00a0[6]: Copied! <pre>us_df['new_cases'] = us_df.cases.diff()\nus_df['new_deaths'] = us_df.deaths.diff()\n\nfig, axes = plt.subplots(2,2, figsize=(11,5), sharex=True)\nfig.suptitle('Cases, Deaths for US Popultion from 3/20 to 3/23')\nfig.autofmt_xdate()\n\nsns.lineplot(ax=axes[0,0], x=us_df.date, y=us_df.cases, hue=us_df.date.dt.year)\nsns.lineplot(ax=axes[0,1], x=us_df.date, y=us_df.deaths, hue=us_df.date.dt.year, legend=None)\nsns.lineplot(ax=axes[1,0], x=us_df.date, y=us_df.new_cases, hue=us_df.date.dt.year, legend=None)\nsns.lineplot(ax=axes[1,1], x=us_df.date, y=us_df.new_deaths, hue=us_df.date.dt.year, legend=None)\n\n#us_df.plot(x='date', subplots=True, layout=(2,2))\nplt.show()\n</pre> us_df['new_cases'] = us_df.cases.diff() us_df['new_deaths'] = us_df.deaths.diff()  fig, axes = plt.subplots(2,2, figsize=(11,5), sharex=True) fig.suptitle('Cases, Deaths for US Popultion from 3/20 to 3/23') fig.autofmt_xdate()  sns.lineplot(ax=axes[0,0], x=us_df.date, y=us_df.cases, hue=us_df.date.dt.year) sns.lineplot(ax=axes[0,1], x=us_df.date, y=us_df.deaths, hue=us_df.date.dt.year, legend=None) sns.lineplot(ax=axes[1,0], x=us_df.date, y=us_df.new_cases, hue=us_df.date.dt.year, legend=None) sns.lineplot(ax=axes[1,1], x=us_df.date, y=us_df.new_deaths, hue=us_df.date.dt.year, legend=None)  #us_df.plot(x='date', subplots=True, layout=(2,2)) plt.show() In\u00a0[36]: Copied! <pre>def years_overlaid(df, column, ax):\n\n    us2020 = df.loc[df.date.dt.year == 2020].reset_index(drop=True)\n    us2021 = df.loc[df.date.dt.year == 2021].reset_index(drop=True)\n    us2022 = df.loc[df.date.dt.year == 2022].reset_index(drop=True)\n    us2023 = df.loc[df.date.dt.year == 2023].reset_index(drop=True)\n\n    sns.lineplot(ax=ax,x=us2020.date.dt.day_of_year,y=us2020[f'{column}'], color='blue')\n    sns.lineplot(ax=ax, x=us2021.date.dt.day_of_year, y=us2021[f'{column}'],color='purple')\n    sns.lineplot(ax=ax, x=us2022.date.dt.day_of_year,y=us2022[f'{column}'],color='red')\n    sns.lineplot(ax=ax, x=us2023.date.dt.day_of_year, y=us2023[f'{column}'],color='orange')\n\n# new columns to smooth out the data\nus_df['cases_7d_avg'] = us_df.new_cases.rolling(7).sum() / 7\nus_df['deaths_7d_avg'] = us_df.new_deaths.rolling(7).sum() / 7\n\nfig, axes = plt.subplots(2,2, figsize=(11,5), sharex=True)\nfig.suptitle('Cases, Deaths: Years Overlaid')\naxes[1,0].set_xlabel('Day of Year')\naxes[1,1].set_xlabel('Day of Year')\n\n\nyears_overlaid(us_df,'cases',ax=axes[0,0])\nyears_overlaid(us_df,'deaths',ax=axes[0,1])\nyears_overlaid(us_df,'cases_7d_avg',ax=axes[1,0])\nyears_overlaid(us_df,'deaths_7d_avg',ax=axes[1,1])\n</pre> def years_overlaid(df, column, ax):      us2020 = df.loc[df.date.dt.year == 2020].reset_index(drop=True)     us2021 = df.loc[df.date.dt.year == 2021].reset_index(drop=True)     us2022 = df.loc[df.date.dt.year == 2022].reset_index(drop=True)     us2023 = df.loc[df.date.dt.year == 2023].reset_index(drop=True)      sns.lineplot(ax=ax,x=us2020.date.dt.day_of_year,y=us2020[f'{column}'], color='blue')     sns.lineplot(ax=ax, x=us2021.date.dt.day_of_year, y=us2021[f'{column}'],color='purple')     sns.lineplot(ax=ax, x=us2022.date.dt.day_of_year,y=us2022[f'{column}'],color='red')     sns.lineplot(ax=ax, x=us2023.date.dt.day_of_year, y=us2023[f'{column}'],color='orange')  # new columns to smooth out the data us_df['cases_7d_avg'] = us_df.new_cases.rolling(7).sum() / 7 us_df['deaths_7d_avg'] = us_df.new_deaths.rolling(7).sum() / 7  fig, axes = plt.subplots(2,2, figsize=(11,5), sharex=True) fig.suptitle('Cases, Deaths: Years Overlaid') axes[1,0].set_xlabel('Day of Year') axes[1,1].set_xlabel('Day of Year')   years_overlaid(us_df,'cases',ax=axes[0,0]) years_overlaid(us_df,'deaths',ax=axes[0,1]) years_overlaid(us_df,'cases_7d_avg',ax=axes[1,0]) years_overlaid(us_df,'deaths_7d_avg',ax=axes[1,1])  <p>This data shown above is all for the US totals only.  In my original script, I drill down further into states and counties. I wont do that here. refer to the link above to explore further down.</p> In\u00a0[8]: Copied! <pre>selected_states = ['California',\n                   'Illinois',\n                   'West Virginia'\n                   ]\n\ndef state_compare(list_of_places, df, column):\n    find_rows = df.loc[df.state.isin(list_of_places)]\n    \n    plt.plot(find_rows[f'{column}'].loc[find_rows.state == list_of_places[0]])\n    plt.plot(find_rows[f'{column}'].loc[find_rows.state == list_of_places[1]])\n    plt.plot(find_rows[f'{column}'].loc[find_rows.state == list_of_places[2]])\n    \nstate_compare(selected_states,state_df,'cases')\n</pre>  selected_states = ['California',                    'Illinois',                    'West Virginia'                    ]  def state_compare(list_of_places, df, column):     find_rows = df.loc[df.state.isin(list_of_places)]          plt.plot(find_rows[f'{column}'].loc[find_rows.state == list_of_places[0]])     plt.plot(find_rows[f'{column}'].loc[find_rows.state == list_of_places[1]])     plt.plot(find_rows[f'{column}'].loc[find_rows.state == list_of_places[2]])      state_compare(selected_states,state_df,'cases') <p>Note: the graph above does not account for population.  For more depth, refer to the link above.</p> <p>At first glance trends looked pretty similar, especially when set against a region\u2019s population. This would need to be tested later against regions I had not selected for personal reasons.</p> <p>One more disclaimer: while I recognize that analytical skills are necessary for any data application, the purpose of this project was to demonstrate data engineering and achitecture principles. I could have done much more statistical analysis, but chose to work more on the engineering side, for this time.</p> In\u00a0[3]: Copied! <pre>Image(filename='covid_erd.png', height=500, width=700)\n</pre> Image(filename='covid_erd.png', height=500, width=700) Out[3]: <p>This allowed me to plan my table relationships before populating the tables. PGAdmin exported the ERD file as a SQL file to build all the tables with their respective keys. So when it was time to build a MySql version of the database, its was mostly about editing that file, and tailoring it to the data specifically.</p>"},{"location":"covid_blog/#project-1-nyt-covid-dashboard","title":"Project 1: NYT Covid Dashboard\u00b6","text":""},{"location":"covid_blog/#phase-1-project-origins","title":"Phase 1: Project Origins\u00b6","text":""},{"location":"covid_blog/#topicsskills-used","title":"Topics/Skills Used\u00b6","text":"<ul> <li>Analysis of NYT covid data over 3 years</li> <li>data cleaning: python/pandas</li> <li>data modeling: star schema architecture</li> <li>large dataset: 3.5 million data points</li> </ul> <p>Like many people, I spent much of 2020 looking at COVID-19 stats.  I would see averages and trends, but I always wanted to see a little more.   I always wanted to see it interpreted in different ways.  I always wanted to see how things compared to poulations (which I later learned were the prevalence and incidence rate).During 2022, I started really digging in and learning python, SQL, and shell scripting.  At the end of my first course, I realized that I could probably interpret the data myself, using the tools I had acquired.</p>"},{"location":"covid_blog/#goals","title":"Goals\u00b6","text":"<p>Here were the main goals from the start:</p> <ul> <li>to compare case trends from different years on the same axis</li> <li>to calculate the prevalence and incidence rate for each region using its population</li> <li>to compare case trends between regions</li> <li>to compare regional outcomes for case numbers against their populations</li> </ul>"},{"location":"covid_blog/#phase-2-gathering-the-data","title":"Phase 2: Gathering the Data\u00b6","text":""},{"location":"covid_blog/#using-requestsget-to-download-data","title":"Using requests.get() to download data\u00b6","text":"<p>Throughout the pandemic, I had mostly used the NYT covid dashboard.  WHile many datasets were out there, the Times' data seemed to attempt to aggregate everyone's results, both by using the Johns Hopkins data and reaching out to local heath authorities (as mentioned in their methodology).  They also include probable cases, which likely accounts for some of the irregularities in county level data.  In a few counties across the nation, consecutive cases dip back down.  The methodolgy does not acknowledge this; but if you look closely in their dashboards, the dates in question do appear with an \"error, anomalous data\" message.</p> <p>The New York Times has stopped their data collection, and thus the data has gone stale.  Curent dashboards use wastewater measurement and hospital occupancy. Still, as historical data, it is useful as a comprehensive dataset to portray case movement over the three years.  And certainly if nothing else, portrays how we saw COVID during that time.</p> <p>The first task was to download the necessary csv files, from the NYT GitHub site..</p>"},{"location":"covid_blog/#phase-3-the-eda","title":"Phase 3: The EDA\u00b6","text":"<p>(FYI: I will not be including the original code examples in this section.  My original eda script was pretty messy and involved a lot more code than was necessary.  All part of the learning process, but I'll spare the details here.  To view the script mentioned here got to https://github.com/cskimm01/NYT_COV_visualize/blob/main/covid_visuals.ipynb).  This was really the first thing I ever coded independently, so please be forgiving if you would.  I knew how to get things to work, mostly. I didn't know any clean code concepts or best practices at that time.  But in the end, it did it's job by getting me started with exploring the dataset.</p>"},{"location":"covid_blog/#first-visualizations-with-pyplot","title":"First Visualizations with Pyplot\u00b6","text":"<p>I started by cherry picking locations I was interested in; My hometown (Kanawha County, WV), where I live now (Cook County, IL), where my family members live, and so on.  And with this I learned how to get the visualizations I wanted, using pyplot from matplotlib.  And my code usually worked, but as I panned out further, I realized many exceptions in the data set.  Not every state has counties: some have parishes, some have boroughs.  Many couties share names (Jackson is the most common county name in the US). Getting a naming convention to work across the board was a challenge. This would be solved later by creating a star schema and assigning unique ids to each state and county.</p>"},{"location":"covid_blog/#aggregations-and-missing-data","title":"Aggregations and Missing Data\u00b6","text":"<p>As I dug a bit deeper, I realized the weird catches in the data.  New York City had been aggregated from its five counties to a whole.  Also, certain parts of Alaska were combined to make the populations more relevant.  This was all spelled out in the NYT Github documentation.</p> <p>There were a number of NULL FIPS numbers.  They mostly fixed by replacing them with a city code.  The only other NULLs were in Puerto Rico,  where deaths were not reported at the municipio (county) level.  Just Zero them out for now, all done.</p> <p>Because of some strange reprting in Missouri, Kansas City and Joplin MI reported separately from the counties they occupy.  New York City posed the same problem, that the city numer were reported, but not a the county level.  So I just aggregated the cities and their counties together, instead of picking apart populations for each.</p>"},{"location":"covid_blog/#phase-4-creating-the-star-schema","title":"Phase 4: Creating the Star Schema\u00b6","text":"<p>If you want follow along, go to https://github.com/cskimm01/NYT_COV_visualize/blob/main/star_schema_final.ipynb And if you want to see every mistake I made along the way, check out https://github.com/cskimm01/NYT_COV_visualize/blob/main/star_schema_build.ipynb</p> <p>After getting the hang of a few of the details of the dataset, I started a new script to parse the county-level data into dimension and fact tables. All of the data from this dataset comes from the four us-county.csv files from the New York Times. The plan was to parse the data into three dimension tables:</p> <ul> <li>dimState</li> <li>dimDate</li> <li>dimCounty</li> </ul> <p>And one fact table to represent the case count and deaths count:</p> <ul> <li>factCases</li> </ul> <p>While they had descibed star schemas in my data engineering course, we never build one from scratch and populated the data.  So , this was all new territory for me.</p> <p>We did however, use the pgAdmin ERD tool. Using the tool, I was able to come up with this:</p>"},{"location":"covid_blog/#phase-5-creating-the-database","title":"Phase 5: Creating the Database\u00b6","text":"<p>Although creating the indexes in Python worked well for creating the csv to load to Tableau, it was my original plan to load them into a SQL database, with the index being set by an autoincrementing column.  But in the name of expedience, and a limited Tableau Public account, I imported the csv files directly into Tableau.  When I went to put the data into my schema, I had not anticipated having the indexes already generated.  So I changed my tables a bit, and the pgAdmin ERD tool made that a breeze.</p> <p>Now I had the schema planned and built in Postgres and MySQL, it was time to poulate the data.  THis took more trial and erro than I would like, but I attempted a few different ways just to explore.  In Postgres, importing was a breeze.  I uploaded two documents with PGAdmin, and two documents using the CLI with the <code>COPY</code> command.</p> <p>MySQL was much trickier.  I spent a lot of time with  failed imports, only to find that I needed to declare my separators and line breaks.  I also had to spend a lot of time digging aroung in the variables to disable security features, such as <code>GLOBAL IN_FILE=1</code> and set the path for <code>secure-file-priv</code> in the my.cnf file (it took me ages to find tha file).</p> <p>Anyhow, I imported two tables via <code>LOAD INFILE</code> and two using <code>mysqlimport</code>, which was difficult but felt important to understand.  I had one weird glitch, though.  I had set up my aggregations a certain way to work with the map in Tableau, so my Aggregated counties were listed as population=0.  While this was good for aggregating the whole stae or groups of states, if I wanted to use my fact table data, I would need the population data for the aggregates.  So real quick, I wrote a few queries that add the data from the states together and udate the population of the aggregates.</p> <p>The one thing I am still working on, is how to keep the aggregates separate from the individual counties.  As of now, my plan is to create a view with the aggregates updated and the states dropped, but I haven't had time yet to implement it.</p> <p>The purpose of the database was to be able to link to Tableau, but I only have Tableau public at the moment and can only work from extracts as sources. But the data is inthere, when someone wants to get to it!</p>"},{"location":"covid_blog/#phase-6-creating-the-dashboard","title":"Phase 6: Creating the Dashboard\u00b6","text":"<p>The dashboard can be found here: https://public.tableau.com/app/profile/chris.kimmons/viz/NYTCovidMap/Dashboard1</p> <p>With everthing indexed and ready to go, it was time to start loading into Tableau.  Tableau's data source UI took some getting used to, and I made some mistakes at first.  But I got my files hooked up with the right connections, and was able to tune the performance options.</p> <p>Time to start making some graphs! But...</p> <p>I had a lot of trial and error getting my visualizations to work.  Matplotlib had actually been easier to work with; or at least seemed easier.  Tableau had a whole different approach with its UI, and so many levels and layers to the structure of the data between the source and the final viz.</p>"},{"location":"covid_blog/#the-map","title":"The Map\u00b6","text":"<p>I started with a cloropleth map of the US mainland, using population to color in the states and counties.  I originally had a very zoomed out approach, but it was difficulat to navigate, so I had to build a fairly complicated dashboard to make navigation easier across al of the US territories. Also, the county data was aggregating incorrectly.  In an early attemnt, ever county with the same name summed together.  But the toughest part was grouping the aggregate counties together to represent on the map properly.  In order to do this, I used the group tool in Tableau to add together the county populations, named the groups to match the dimCounty table, and used the grouping for almost every sheet forward.</p>"},{"location":"covid_blog/#the-plots","title":"The Plots\u00b6","text":"<p>I went a bit further in the analysis with the line plots in Tableau.  I added calculations for prevalence, incidence rate, and case-to-death ratio over time. I finally got my plots over-time and my years_overlaid tables, but I wanted to do more with the map. So created some LOD calculations to color the states by prevalence, deaths, and final death-to-case ratio.</p> <p>Plotting the years over each other took a lot of time and reworking.  Luckily, I was able to figure out how to do it within one case statement and using the day_of_year column.</p> <p>I had a good bit of fortune to meet Abigail Cartus, MPH PhD.  She was generous enought o look over my figures and make sure everything was (mostly) statistacally sound and if anything need to be brushed up. Thanks again Abby!</p>"},{"location":"covid_blog/#interactivity","title":"Interactivity\u00b6","text":"<p>The big map was not efficient for selecting at the county level, so I need a way to drill down.  I found a tutorial on datavizjen.com tha was able to get me started on the concepts that would allow me to toggle between state level and county level.  It involved some very specific filtering, and many complex actions allowing the user to navigate by clicking in the dash board.  Actions are the first thing to show signs of lag, so I had to be very intent with what data I used to keep the speed up.  Also, it involved making a sheet for each territory, which was also a drain on performance, but was really necessary for a clean look.</p> <p>I used parameters to select the different calculations on the data.  This way, the user could select which calculations they chose to view.  The calculations can be selected in the drop-down boxes on the right.  Setting up the capculated field for these was not easy, and required more characters than Tableu recommended. But this seemed like the most intuitive way to explore the dat, so I stuck with it.</p>"},{"location":"covid_blog/#refactoring","title":"Refactoring\u00b6","text":"<p>After all of this, my dashboard was usually showing at least 2200 marks. In otherwords, it was pretty laggy when performing actions. So I had to find ways to make it leaner. Adding filters to the context helped dramtically.  Also, decreasing the marks on the dashboard as much as possible without sacrificing the visuals.</p> <p>An interesting approach I tried, that I'm not sure if it worked or not, was to use the date_id column every time istead of the full date.  This kept the querying to integers, and not having to parse datetypes. I couldn't convert the ticks to another field, however, so hid the ticks and added the full date to the tooltip so you can see it as you hover over the plot.</p> <p>Overall, the dashboard runs ok, seeing as its going thro 3.5 million rows with each query. Still, I want it faster...</p>"},{"location":"covid_blog/#final-takeaways","title":"Final Takeaways\u00b6","text":"<p>This was the biggest project I have done so far, and it had so many components to it.  Boiling it down is going to be tough, but here goes:</p> <ul> <li>Decide on good column names from the start, and load them from a variable.  Otherwise you will jusping around forever updating your column names</li> <li>Learn more statistics! It will help with more thourough EDA</li> <li>Tableau needs the data to be a certain way to keep it running smoothly</li> <li>Check your indexes twice.  Make sure your data is merging properly</li> <li>Always check data types.  You will waste a lot of time not setting them from the start</li> <li>Always declare your delimiters, especially in MySQL.  I used up so much time in <code>mysqlimport</code> trying to find what was wrong with my command</li> </ul> <p>A lot of these concepts are on display in this notebook, which was coded entirely separately from anything in the NYT_Covid repositiory.  This gave me an opportunity to reflect on what I had learned, and test myself on how fast and how clearly I could code my concepts.</p> <p>This has been a huge project for me, and I am happy to let it rest now, at least for a little while...</p>"},{"location":"durrp/","title":"DURRP:  An alternative time keeping solution","text":"<p>DURRP:  An alternative time keeping solution</p> In\u00a0[\u00a0]: Copied! <pre>'''\nThis script is basically a software recreation of the DURR watch, by the Scandinavian design team Skrekkogle. \nDetails can be found at http://skrekkogle.com/projects/durr/.\n\nAs someone who has ADHD, this is the kind of tool I had always wanted as a kid. \nEvery five minutes it makes a sound and delivers an encouraging message, to help \nremind you how quickly your time is passing.  Skrekogle only produced a limited amount \nof the hardware version, so they are no longer available. So I made my own software version!\n\nYou can start it yourself everyday or use chron to automate.\n'''\n</pre> ''' This script is basically a software recreation of the DURR watch, by the Scandinavian design team Skrekkogle.  Details can be found at http://skrekkogle.com/projects/durr/.  As someone who has ADHD, this is the kind of tool I had always wanted as a kid.  Every five minutes it makes a sound and delivers an encouraging message, to help  remind you how quickly your time is passing.  Skrekogle only produced a limited amount  of the hardware version, so they are no longer available. So I made my own software version!  You can start it yourself everyday or use chron to automate. ''' In\u00a0[\u00a0]: Copied! <pre>import time\nimport subprocess\nimport sys\n</pre> import time import subprocess import sys In\u00a0[\u00a0]: Copied! <pre>def show_notification(title, message):\n    # Use AppleScript with a timeout option to clear the notification after 60 seconds (1 minute)\n    script = f'display notification \"{title}\" with title \"{message}\" subtitle \"You\\'ve Got This!\" sound name \"Submarine\"'\n    subprocess.run(['osascript', '-e', script])\n</pre> def show_notification(title, message):     # Use AppleScript with a timeout option to clear the notification after 60 seconds (1 minute)     script = f'display notification \"{title}\" with title \"{message}\" subtitle \"You\\'ve Got This!\" sound name \"Submarine\"'     subprocess.run(['osascript', '-e', script]) In\u00a0[\u00a0]: Copied! <pre>### main body\ndef main():\n\n    current_hour = int(time.strftime(\"%H\"))\n\n    try:\n        start_hour = int(sys.argv[1])\n        end_hour = int(sys.argv[2])\n    except IndexError:\n        start_hour = current_hour\n        end_hour = 17\n\n    # small function to determine AM and PM\n    am_pm = lambda hour: 'AM' if hour &lt; 12 else 'PM'\n\n    print(f'durr.py running at hour {current_hour%12}{am_pm(current_hour)}, ending at {end_hour%12}{am_pm(end_hour)} \\n press ^+c to end')\n\n    while True:\n        \n        current_hour = int(time.strftime(\"%H\"))\n        if start_hour &lt;= current_hour &lt; end_hour:\n            show_notification(\"\\'5 Minutes Has Passed!\\'\", \"\\'DURRP\\'\")\n            time.sleep(300)  # 300 seconds = 5 minutes\n        else:\n            time.sleep(30)  # 60 seconds = 1 minute\n\n        if current_hour &gt;= end_hour:\n            print(f'durr.py terminating at {end_hour}')\n            break\n</pre> ### main body def main():      current_hour = int(time.strftime(\"%H\"))      try:         start_hour = int(sys.argv[1])         end_hour = int(sys.argv[2])     except IndexError:         start_hour = current_hour         end_hour = 17      # small function to determine AM and PM     am_pm = lambda hour: 'AM' if hour &lt; 12 else 'PM'      print(f'durr.py running at hour {current_hour%12}{am_pm(current_hour)}, ending at {end_hour%12}{am_pm(end_hour)} \\n press ^+c to end')      while True:                  current_hour = int(time.strftime(\"%H\"))         if start_hour &lt;= current_hour &lt; end_hour:             show_notification(\"\\'5 Minutes Has Passed!\\'\", \"\\'DURRP\\'\")             time.sleep(300)  # 300 seconds = 5 minutes         else:             time.sleep(30)  # 60 seconds = 1 minute          if current_hour &gt;= end_hour:             print(f'durr.py terminating at {end_hour}')             break In\u00a0[\u00a0]: Copied! <pre>### run\nif __name__ == \"__main__\":\n\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(f': durrp.py terminated by user at {time.strftime(\"%I\")}:{time.strftime(\"%M%p\")}')\n</pre> ### run if __name__ == \"__main__\":      try:         main()     except KeyboardInterrupt:         print(f': durrp.py terminated by user at {time.strftime(\"%I\")}:{time.strftime(\"%M%p\")}')"},{"location":"durrp/#about","title":"About\u00b6","text":""},{"location":"durrp/#body","title":"Body\u00b6","text":""},{"location":"Melody_Graph/gab/","title":"gab: a module for parsing music21 streams","text":"<p>gab: a module for parsing music21 streams</p> In\u00a0[\u00a0]: Copied! <pre>'''\nmodule from Jamie Gabriel at https://github.com/jgab3103\n'''\n</pre> ''' module from Jamie Gabriel at https://github.com/jgab3103 ''' In\u00a0[\u00a0]: Copied! <pre>import librosa\nfrom colorsys import rgb_to_hsv, hsv_to_rgb\nimport music21 as mu\nfrom midi2audio import FluidSynth\nimport pandas as pd\nimport numpy as np\nimport sympy as sp\nfrom IPython.display import HTML, IFrame\nimport IPython\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom matplotlib.patches import Rectangle\nfrom IPython.display import Image\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Patch\n</pre> import librosa from colorsys import rgb_to_hsv, hsv_to_rgb import music21 as mu from midi2audio import FluidSynth import pandas as pd import numpy as np import sympy as sp from IPython.display import HTML, IFrame import IPython import matplotlib.pyplot as plt from matplotlib import animation from matplotlib.patches import Rectangle from IPython.display import Image from matplotlib.lines import Line2D from matplotlib.patches import Patch In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>def create_sound_file_from_midi(m21_data, file_name):\n    m21_data.write('midi', fp = file_name + \".mid\")\n    fs = FluidSynth()\n    fs.midi_to_audio('./' + file_name + \".mid\", file_name + '.mp3')\n    return(IPython.display.Audio(\"./\" + file_name + \".mp3\"))\n</pre> def create_sound_file_from_midi(m21_data, file_name):     m21_data.write('midi', fp = file_name + \".mid\")     fs = FluidSynth()     fs.midi_to_audio('./' + file_name + \".mid\", file_name + '.mp3')     return(IPython.display.Audio(\"./\" + file_name + \".mp3\")) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>def complementary(r, g, b):\n    hsv = rgb_to_hsv(r, g, b)\n    return hsv_to_rgb((hsv[0] + 0.5) % 1, hsv[1], hsv[2])\n</pre> def complementary(r, g, b):     hsv = rgb_to_hsv(r, g, b)     return hsv_to_rgb((hsv[0] + 0.5) % 1, hsv[1], hsv[2]) In\u00a0[\u00a0]: Copied! <pre>def createMatPlotLibAxisForScore(ax, x_limit, y_limit):    \n    ax.set_xticks(range(int(x_limit)))\n    ax.set_yticks(range(int(y_limit)))\n    [ax.xaxis.get_major_ticks()[i].tick1line.set_color(\"white\") for i in range(int(x_limit))]\n    [ax.yaxis.get_major_ticks()[i].tick1line.set_color(\"white\") for i in range(int(y_limit))]\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n    ax.grid(color='k', linestyle='-', linewidth=.5)\n    ax.tick_params(axis = \"both\", which = \"both\", bottom = False, top = False)\n    \n    return(ax)\n</pre> def createMatPlotLibAxisForScore(ax, x_limit, y_limit):         ax.set_xticks(range(int(x_limit)))     ax.set_yticks(range(int(y_limit)))     [ax.xaxis.get_major_ticks()[i].tick1line.set_color(\"white\") for i in range(int(x_limit))]     [ax.yaxis.get_major_ticks()[i].tick1line.set_color(\"white\") for i in range(int(y_limit))]     ax.set_yticklabels([])     ax.set_xticklabels([])     ax.grid(color='k', linestyle='-', linewidth=.5)     ax.tick_params(axis = \"both\", which = \"both\", bottom = False, top = False)          return(ax) In\u00a0[\u00a0]: Copied! <pre>def createScoreFormatting(ax1, yOffset):\n    \n    # formatting data\n    text_kwargs = dict( fontsize=32, color='white')\n    \n    keyColorCoords = [((0, 1), 80, 1), ((0, 3), 80, 1), ((0, 6), 80, 1), ((0, 8), 80, 1), ((0, 10), 80, 1), \n                 ((0, 13), 80, 1),((0, 15), 80, 1),((0, 18), 80, 1),((0, 20), 80, 1),((0, 22), 80, 1),\n                 ((0, 25), 80, 1), ((0, 27), 80, 1), ((0, 30), 80, 1), ((0, 32), 80, 1), ((0, 34), 80, 1),\n                 ((0, 37), 80, 1), ((0, 39), 80, 1), ((0, 42), 80, 1), ((0, 44), 80, 1), ((0, 46), 80, 1),\n                 ((0, 49), 80, 1), ((0, 51), 80, 1), ((0, 54), 80, 1), ((0, 56), 80, 1), ((0, 58), 80, 1),\n                 ((0, 61), 80, 1), ((0, 63), 80, 1), ((0, 66), 80, 1), ((0, 68), 80, 1), ((0, 70), 80, 1),\n                 ((0, 73), 80, 1), ((0, 75), 80, 1), ((0, 78), 80, 1), ((0, 80), 80, 1), ((0, 82), 80, 1),\n                 ((0, 85), 80, 1), ((0, 87), 80, 1), ((0, 90), 80, 1), ((0, 92), 80, 1), ((0, 94), 80, 1),\n                 ((0, 97), 80, 1), ((0, 99), 80, 1), ((0, 102), 80, 1), ((0, 104), 80, 1), ((0, 106), 80, 1),\n                 ((0, 109), 80, 1), ((0, 111), 80, 1), ((5, 114), 80, 1), ((0, 116), 80, 1), ((0, 118), 80, 1)]\n\n\n    \n    # Draw black and white keys\n    [ax1.add_patch(Rectangle((keyColorCoords[i][0][0],keyColorCoords[i][0][1] - yOffset), keyColorCoords[i][1], keyColorCoords[i][2], color=\"#EBECF0\", zorder = -10)) for i in range(len(keyColorCoords))]\n\n    ax1.add_patch(Rectangle((74, 0), 10, 80, color = \"black\"))\n    \n\n    return(ax1)\n</pre> def createScoreFormatting(ax1, yOffset):          # formatting data     text_kwargs = dict( fontsize=32, color='white')          keyColorCoords = [((0, 1), 80, 1), ((0, 3), 80, 1), ((0, 6), 80, 1), ((0, 8), 80, 1), ((0, 10), 80, 1),                   ((0, 13), 80, 1),((0, 15), 80, 1),((0, 18), 80, 1),((0, 20), 80, 1),((0, 22), 80, 1),                  ((0, 25), 80, 1), ((0, 27), 80, 1), ((0, 30), 80, 1), ((0, 32), 80, 1), ((0, 34), 80, 1),                  ((0, 37), 80, 1), ((0, 39), 80, 1), ((0, 42), 80, 1), ((0, 44), 80, 1), ((0, 46), 80, 1),                  ((0, 49), 80, 1), ((0, 51), 80, 1), ((0, 54), 80, 1), ((0, 56), 80, 1), ((0, 58), 80, 1),                  ((0, 61), 80, 1), ((0, 63), 80, 1), ((0, 66), 80, 1), ((0, 68), 80, 1), ((0, 70), 80, 1),                  ((0, 73), 80, 1), ((0, 75), 80, 1), ((0, 78), 80, 1), ((0, 80), 80, 1), ((0, 82), 80, 1),                  ((0, 85), 80, 1), ((0, 87), 80, 1), ((0, 90), 80, 1), ((0, 92), 80, 1), ((0, 94), 80, 1),                  ((0, 97), 80, 1), ((0, 99), 80, 1), ((0, 102), 80, 1), ((0, 104), 80, 1), ((0, 106), 80, 1),                  ((0, 109), 80, 1), ((0, 111), 80, 1), ((5, 114), 80, 1), ((0, 116), 80, 1), ((0, 118), 80, 1)]            # Draw black and white keys     [ax1.add_patch(Rectangle((keyColorCoords[i][0][0],keyColorCoords[i][0][1] - yOffset), keyColorCoords[i][1], keyColorCoords[i][2], color=\"#EBECF0\", zorder = -10)) for i in range(len(keyColorCoords))]      ax1.add_patch(Rectangle((74, 0), 10, 80, color = \"black\"))           return(ax1) In\u00a0[\u00a0]: Copied! <pre>def convertDataForScoreVisualisation(scoreDataAsDF, startMeasure, endMeasure):\n\n    df1 = scoreDataAsDF[(scoreDataAsDF.measureNumber &gt;= startMeasure) &amp; (scoreDataAsDF.measureNumber &lt;= endMeasure) &amp; (scoreDataAsDF.midiNumber != -1)]\n    offsetForStart = df1.offsetAsFloat.min() # min(df1.offsetAsFloat)\n    minMidiNumber =  df1.midiNumber.min() # min(df1.midiNumber)\n    maxMidiNumber = df1.midiNumber.max()  # max(df1.midiNumber)\n   \n    df2 = df1[[\"instrument\", \"part\",\"measureNumber\", \"offsetAsFloat\", \"midiNumber\", \"nameWithOctave\", \"quarterLengthDurationAsFloat\", \"partColor\"]].copy().reset_index()\n\n    df2['height'] = 1\n    heightNormalize = df2.groupby(by=[\"offsetAsFloat\", \"midiNumber\"]).sum().reset_index()[['offsetAsFloat', 'midiNumber', 'height']]\n    new_df = pd.merge(df2, heightNormalize,  how='left', left_on=['offsetAsFloat','midiNumber'], right_on = ['offsetAsFloat','midiNumber'])\n    df = new_df.sort_values(by = [ \"offsetAsFloat\", \"midiNumber\"]).reset_index()\n    df[\"divider\"] = df.groupby((df[\"height_y\"]!=df[\"height_y\"].shift()).cumsum()).cumcount() + 1\n    df['divider'] = np.where(df.height_y == 1,1, df.divider)\n    df['width'] = df.height_x / df.height_y\n    \n    \n    df['adjustedHeightOffset'] = ((df.height_x / df.height_y) * df.divider) - df.width\n    \n    df['new_col'] = list(zip(df[\"offsetAsFloat\"]-offsetForStart, (df[\"midiNumber\"] - minMidiNumber) + df['adjustedHeightOffset'] ))\n    df['col2'] = list(zip(df[\"new_col\"], df[\"quarterLengthDurationAsFloat\"], df[\"width\"], df[\"partColor\"], df['nameWithOctave']))\n    \n\n    coords = df.col2.values\n\n    return([coords, minMidiNumber, maxMidiNumber, df])\n</pre> def convertDataForScoreVisualisation(scoreDataAsDF, startMeasure, endMeasure):      df1 = scoreDataAsDF[(scoreDataAsDF.measureNumber &gt;= startMeasure) &amp; (scoreDataAsDF.measureNumber &lt;= endMeasure) &amp; (scoreDataAsDF.midiNumber != -1)]     offsetForStart = df1.offsetAsFloat.min() # min(df1.offsetAsFloat)     minMidiNumber =  df1.midiNumber.min() # min(df1.midiNumber)     maxMidiNumber = df1.midiNumber.max()  # max(df1.midiNumber)         df2 = df1[[\"instrument\", \"part\",\"measureNumber\", \"offsetAsFloat\", \"midiNumber\", \"nameWithOctave\", \"quarterLengthDurationAsFloat\", \"partColor\"]].copy().reset_index()      df2['height'] = 1     heightNormalize = df2.groupby(by=[\"offsetAsFloat\", \"midiNumber\"]).sum().reset_index()[['offsetAsFloat', 'midiNumber', 'height']]     new_df = pd.merge(df2, heightNormalize,  how='left', left_on=['offsetAsFloat','midiNumber'], right_on = ['offsetAsFloat','midiNumber'])     df = new_df.sort_values(by = [ \"offsetAsFloat\", \"midiNumber\"]).reset_index()     df[\"divider\"] = df.groupby((df[\"height_y\"]!=df[\"height_y\"].shift()).cumsum()).cumcount() + 1     df['divider'] = np.where(df.height_y == 1,1, df.divider)     df['width'] = df.height_x / df.height_y               df['adjustedHeightOffset'] = ((df.height_x / df.height_y) * df.divider) - df.width          df['new_col'] = list(zip(df[\"offsetAsFloat\"]-offsetForStart, (df[\"midiNumber\"] - minMidiNumber) + df['adjustedHeightOffset'] ))     df['col2'] = list(zip(df[\"new_col\"], df[\"quarterLengthDurationAsFloat\"], df[\"width\"], df[\"partColor\"], df['nameWithOctave']))           coords = df.col2.values      return([coords, minMidiNumber, maxMidiNumber, df]) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>def visualizeScore(scoreData, fromMeasure = None, toMeasure = None, reduce = False):\n    \n    coords1 = convertDataForScoreVisualisation(scoreData, fromMeasure, toMeasure)\n\n    text_kwargs = dict( fontsize=10, color='orange')\n\n    legend_elements = [Line2D([0], [0], color='b', lw=4, label='Violin'),\n                       Line2D([0], [0], marker='o', color='w', label='Viola',\n                              markerfacecolor='g', markersize=15),\n                       Patch(facecolor='orange', edgecolor='r',\n                             label='Cello')]\n\n\n\n    # DRAW NOTES\n    fig, (ax1) = plt.subplots(1, 1, figsize=(20, 12))\n\n    ax1 = createMatPlotLibAxisForScore(ax1, 16, (coords1[2] - coords1[1]) + 5)\n\n    t = [ax1.add_patch(Rectangle(coords1[0][i][0], coords1[0][i][1], coords1[0][i][2], color = coords1[0][i][3])) for i in range(len(coords1[0]))]\n\n    u = [ax1.text(coords1[0][j][0][0], coords1[0][j][0][1], coords1[0][j][4], **text_kwargs) for j in range(len(coords1[0]))]\n\n\n    ax1.legend(handles=legend_elements, loc='upper right')\n\n    f1 = createScoreFormatting(ax1, coords1[1])\n</pre> def visualizeScore(scoreData, fromMeasure = None, toMeasure = None, reduce = False):          coords1 = convertDataForScoreVisualisation(scoreData, fromMeasure, toMeasure)      text_kwargs = dict( fontsize=10, color='orange')      legend_elements = [Line2D([0], [0], color='b', lw=4, label='Violin'),                        Line2D([0], [0], marker='o', color='w', label='Viola',                               markerfacecolor='g', markersize=15),                        Patch(facecolor='orange', edgecolor='r',                              label='Cello')]        # DRAW NOTES     fig, (ax1) = plt.subplots(1, 1, figsize=(20, 12))      ax1 = createMatPlotLibAxisForScore(ax1, 16, (coords1[2] - coords1[1]) + 5)      t = [ax1.add_patch(Rectangle(coords1[0][i][0], coords1[0][i][1], coords1[0][i][2], color = coords1[0][i][3])) for i in range(len(coords1[0]))]      u = [ax1.text(coords1[0][j][0][0], coords1[0][j][0][1], coords1[0][j][4], **text_kwargs) for j in range(len(coords1[0]))]       ax1.legend(handles=legend_elements, loc='upper right')      f1 = createScoreFormatting(ax1, coords1[1]) In\u00a0[\u00a0]: Copied! <pre># THIS FUNCTION ASSUMES THAT YOU HAVE PARSED YOUR MUSICXML FILE USING THE COVERTER.PARSE MUSIC21 FUNCTION. \n# ONCE YOU HAVE CREATED PARSED TO A MUSIC21 OBJECT, JUST PASS THAT OBJECT INTO THIS FUNCTION\ndef convertScoreToDF(scoreData = None, scoreName = None, scoreMovement = None):\n    # examine the list of parts\n    partList = scoreData.getElementsByClass(mu.stream.Part)\n    pList = []\n    for i in range(0, len(partList)):\n        pList.append(partList[i])\n        \n        \n    events = []\n\n    currentNumerator = None\n    currentDenominator = None\n    currentInstrument = None\n    currentInstrumentName = None\n    currentPartName = None\n\n\n    for eachPart in pList:\n\n\n        for el in eachPart.flatten():\n        # note that there are all kinds of that you get hold of when iterating through this object - some of these are below (such as pitchname, midi number etc, but by looking at the music21Object you can see all the other info is available) \n            eventDictionary = {}\n            eventDictionary['offset'] = el.offset\n            eventDictionary['quarterLengthDuration'] = el.duration.quarterLength\n            eventDictionary['measureNumber'] = el.measureNumber\n            eventDictionary['currentNumerator'] = currentNumerator\n            eventDictionary['currentDenominator'] = currentDenominator\n            eventDictionary['instrument'] = currentInstrumentName\n            eventDictionary['part'] = currentPartName\n\n\n            currentType = str(type(el))\n\n            if currentType == \"&lt;class 'music21.meter.base.TimeSignature'&gt;\":\n\n                currentNumerator = el.numerator\n                currentDenominator = el.denominator\n\n            if \"instrument\" in currentType:\n\n                currentInstrumentName = el.instrumentName\n                currentPartName = el.partName\n\n            if currentType == \"&lt;class 'music21.note.Rest'&gt;\":\n                #print(\"REST\")\n                eventDictionary['nameWithOctave'] = \"NA\"\n                eventDictionary['midiNumber'] = -1\n                eventDictionary['fullName'] = \"Rest\"\n                eventDictionary['name'] = \"NA\"\n                eventDictionary['octave'] = \"NA\"\n                if el.expressions:\n                    eventDictionary['expression'] = el.expressions[0].name\n                events.append(eventDictionary)\n\n\n            if currentType == \"&lt;class 'music21.note.Note'&gt;\":\n                eventDictionary['nameWithOctave'] = el.nameWithOctave\n                eventDictionary['midiNumber'] = el.pitches[0].midi\n                eventDictionary['fullName'] = el.pitches[0].fullName\n                eventDictionary['name'] = el.pitches[0].name\n                eventDictionary['octave'] = el.pitches[0].octave\n                if el.expressions:\n                    eventDictionary['expression'] = el.expressions[0].name\n                events.append(eventDictionary)\n\n            elif currentType == \"&lt;class 'music21.chord.Chord'&gt;\":\n      \n\n                for eachNote in el:\n             \n                    tempEventDictionary = eventDictionary.copy()\n\n                    tempEventDictionary['nameWithOctave'] = eachNote.nameWithOctave\n                    tempEventDictionary['midiNumber'] = eachNote.pitches[0].midi\n                    tempEventDictionary['fullName'] = eachNote.pitches[0].fullName\n                    tempEventDictionary['name'] = eachNote.pitches[0].name\n                    tempEventDictionary['octave'] = eachNote.pitches[0].octave\n                    if el.expressions:\n                        tempEventDictionary['expression'] = el.expressions[0].name\n                    events.append(tempEventDictionary)\n\n            # if currentType == \"&lt;music21.expressions.Tremolo&gt;\":\n            #     eventDictionary['expressions'] = el.expressions[0].name\n           \n                    \n    scoreEventData = pd.DataFrame(events)\n    scoreEventData['offsetAsFloat'] = scoreEventData['offset'].astype(float)\n    scoreEventData['quarterLengthDurationAsFloat'] = scoreEventData.quarterLengthDuration.astype(float)\n    cmap = plt.get_cmap('viridis')\n    colors = cmap(np.linspace(0, 1, len(scoreEventData.part)))\n    scoreEventData['partColor'] = colors.tolist()\n    scoreEventData['scoreName'] = scoreName\n    scoreEventData['movement'] = scoreMovement\n    \n    return(scoreEventData)\n</pre> # THIS FUNCTION ASSUMES THAT YOU HAVE PARSED YOUR MUSICXML FILE USING THE COVERTER.PARSE MUSIC21 FUNCTION.  # ONCE YOU HAVE CREATED PARSED TO A MUSIC21 OBJECT, JUST PASS THAT OBJECT INTO THIS FUNCTION def convertScoreToDF(scoreData = None, scoreName = None, scoreMovement = None):     # examine the list of parts     partList = scoreData.getElementsByClass(mu.stream.Part)     pList = []     for i in range(0, len(partList)):         pList.append(partList[i])                       events = []      currentNumerator = None     currentDenominator = None     currentInstrument = None     currentInstrumentName = None     currentPartName = None       for eachPart in pList:           for el in eachPart.flatten():         # note that there are all kinds of that you get hold of when iterating through this object - some of these are below (such as pitchname, midi number etc, but by looking at the music21Object you can see all the other info is available)              eventDictionary = {}             eventDictionary['offset'] = el.offset             eventDictionary['quarterLengthDuration'] = el.duration.quarterLength             eventDictionary['measureNumber'] = el.measureNumber             eventDictionary['currentNumerator'] = currentNumerator             eventDictionary['currentDenominator'] = currentDenominator             eventDictionary['instrument'] = currentInstrumentName             eventDictionary['part'] = currentPartName               currentType = str(type(el))              if currentType == \"\":                  currentNumerator = el.numerator                 currentDenominator = el.denominator              if \"instrument\" in currentType:                  currentInstrumentName = el.instrumentName                 currentPartName = el.partName              if currentType == \"\":                 #print(\"REST\")                 eventDictionary['nameWithOctave'] = \"NA\"                 eventDictionary['midiNumber'] = -1                 eventDictionary['fullName'] = \"Rest\"                 eventDictionary['name'] = \"NA\"                 eventDictionary['octave'] = \"NA\"                 if el.expressions:                     eventDictionary['expression'] = el.expressions[0].name                 events.append(eventDictionary)               if currentType == \"\":                 eventDictionary['nameWithOctave'] = el.nameWithOctave                 eventDictionary['midiNumber'] = el.pitches[0].midi                 eventDictionary['fullName'] = el.pitches[0].fullName                 eventDictionary['name'] = el.pitches[0].name                 eventDictionary['octave'] = el.pitches[0].octave                 if el.expressions:                     eventDictionary['expression'] = el.expressions[0].name                 events.append(eventDictionary)              elif currentType == \"\":                         for eachNote in el:                                   tempEventDictionary = eventDictionary.copy()                      tempEventDictionary['nameWithOctave'] = eachNote.nameWithOctave                     tempEventDictionary['midiNumber'] = eachNote.pitches[0].midi                     tempEventDictionary['fullName'] = eachNote.pitches[0].fullName                     tempEventDictionary['name'] = eachNote.pitches[0].name                     tempEventDictionary['octave'] = eachNote.pitches[0].octave                     if el.expressions:                         tempEventDictionary['expression'] = el.expressions[0].name                     events.append(tempEventDictionary)              # if currentType == \"\":             #     eventDictionary['expressions'] = el.expressions[0].name                                      scoreEventData = pd.DataFrame(events)     scoreEventData['offsetAsFloat'] = scoreEventData['offset'].astype(float)     scoreEventData['quarterLengthDurationAsFloat'] = scoreEventData.quarterLengthDuration.astype(float)     cmap = plt.get_cmap('viridis')     colors = cmap(np.linspace(0, 1, len(scoreEventData.part)))     scoreEventData['partColor'] = colors.tolist()     scoreEventData['scoreName'] = scoreName     scoreEventData['movement'] = scoreMovement          return(scoreEventData) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>## Beginnings of code for midi conversion\nfrom midiutil import MIDIFile\n</pre> ## Beginnings of code for midi conversion from midiutil import MIDIFile In\u00a0[\u00a0]: Copied! <pre>degrees  = [60, 62, 64, 65, 67, 69, 71, 72] # MIDI note number\ntrack    = 0\nchannel  = 0\ntime     = 0   # In beats\nduration = 1   # In beats\ntempo    = 60  # In BPM\nvolume   = 100 # 0-127, as per the MIDI standard\n</pre> degrees  = [60, 62, 64, 65, 67, 69, 71, 72] # MIDI note number track    = 0 channel  = 0 time     = 0   # In beats duration = 1   # In beats tempo    = 60  # In BPM volume   = 100 # 0-127, as per the MIDI standard In\u00a0[\u00a0]: Copied! <pre>MyMIDI = MIDIFile(1) # One track, defaults to format 1 (tempo track\n                     # automatically created)\nMyMIDI.addTempo(track,time, tempo)\n</pre> MyMIDI = MIDIFile(1) # One track, defaults to format 1 (tempo track                      # automatically created) MyMIDI.addTempo(track,time, tempo) In\u00a0[\u00a0]: Copied! <pre>for pitch in degrees:\n    MyMIDI.addNote(track, channel, pitch, time, duration, volume)\n    time = time + 1\n</pre> for pitch in degrees:     MyMIDI.addNote(track, channel, pitch, time, duration, volume)     time = time + 1 In\u00a0[\u00a0]: Copied! <pre>with open(\"major-scale.mid\", \"wb\") as output_file:\n    MyMIDI.writeFile(output_file)\n</pre> with open(\"major-scale.mid\", \"wb\") as output_file:     MyMIDI.writeFile(output_file) <p>fs = FluidSynth() fs.midi_to_audio('./major-scale.mid', 'again.mp3') import IPython</p> <p>IPython.display.Audio('./again.mp3')</p>"},{"location":"Melody_Graph/graphing_melodies/","title":"Graphing melodies","text":"<p>This script takes midi data from a .mxl (musicxml) file and interprets it.  My plan is to refactor this into a class where these functions can be called as methods.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport gab\nimport music21 as mu\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport os\nimport glob\n%matplotlib inline\n</pre>  import numpy as np import gab import music21 as mu import pandas as pd from matplotlib import pyplot as plt import os import glob %matplotlib inline   In\u00a0[2]: Copied! <pre>def play(stream):\n    sp = mu.midi.realtime.StreamPlayer(stream)\n    sp.play()\n</pre> def play(stream):     sp = mu.midi.realtime.StreamPlayer(stream)     sp.play() In\u00a0[3]: Copied! <pre>def plot_staff(clef, max_value):\n    \n    treble_clef = [77,74,71,67,64]\n    bass_clef = [57,53,50,47,43]\n    midilist = bass_clef + treble_clef\n\n    for i in midilist:\n        plt.plot((0,max_value),(i,i), color='black')\n    \n    plt.plot((0,max_value),(60,60), color='black', ls='-.')\n\ndef graph_melodies(df, show_rests=False):\n    \n    if show_rests is False:\n        df = df.query('midiNumber != -1')\n\n    if df.measureNumber.min() == 0:\n        first_measure = 1\n    else: first_measure = 0\n\n    plt.figure().set_figwidth(20)\n    plt.yticks(ticks=np.arange(36,88,step=12))  # labels=['C3','C4(mid)','C5']\n    plt.xticks(ticks=np.arange(first_measure, df.offset.max(),step=4))\n    plot_staff('grand staff',df.offset.max())\n\n    separate_parts = df.part.unique()\n\n    if None in separate_parts:\n        df.loc[df.part.isnull(), 'part'] = 'part1'\n        separate_parts = df.part.unique()\n\n    if len(separate_parts) == 1:\n        the_split = df.index[(df.offset == df.offset.min()) &amp; (df.index &gt; 0)].min()\n        df.loc[:the_split-1,'part'] = 'right_hand'\n        df.loc[the_split:,'part'] = 'left_hand'\n        separate_parts = df.part.unique()\n        \n    for part in separate_parts:\n        plot_df = df.query('part == @part')\n        plt.plot(plot_df.offset, plot_df.midiNumber, label=f'{part}', marker='o',markersize=5)\n    \n    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n    plt.tight_layout()\n    plt.show()\n\ndef interpret(file):\n    score_stream = mu.converter.parse(file)\n    score_df = gab.convertScoreToDF(score_stream)\n    graph_melodies(score_df)\n    score_stream.show()\n    play(score_stream)\n    # this needs to be a class!!!!!\n</pre> def plot_staff(clef, max_value):          treble_clef = [77,74,71,67,64]     bass_clef = [57,53,50,47,43]     midilist = bass_clef + treble_clef      for i in midilist:         plt.plot((0,max_value),(i,i), color='black')          plt.plot((0,max_value),(60,60), color='black', ls='-.')  def graph_melodies(df, show_rests=False):          if show_rests is False:         df = df.query('midiNumber != -1')      if df.measureNumber.min() == 0:         first_measure = 1     else: first_measure = 0      plt.figure().set_figwidth(20)     plt.yticks(ticks=np.arange(36,88,step=12))  # labels=['C3','C4(mid)','C5']     plt.xticks(ticks=np.arange(first_measure, df.offset.max(),step=4))     plot_staff('grand staff',df.offset.max())      separate_parts = df.part.unique()      if None in separate_parts:         df.loc[df.part.isnull(), 'part'] = 'part1'         separate_parts = df.part.unique()      if len(separate_parts) == 1:         the_split = df.index[(df.offset == df.offset.min()) &amp; (df.index &gt; 0)].min()         df.loc[:the_split-1,'part'] = 'right_hand'         df.loc[the_split:,'part'] = 'left_hand'         separate_parts = df.part.unique()              for part in separate_parts:         plot_df = df.query('part == @part')         plt.plot(plot_df.offset, plot_df.midiNumber, label=f'{part}', marker='o',markersize=5)          plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')     plt.tight_layout()     plt.show()  def interpret(file):     score_stream = mu.converter.parse(file)     score_df = gab.convertScoreToDF(score_stream)     graph_melodies(score_df)     score_stream.show()     play(score_stream)     # this needs to be a class!!!!!       <p>These are midi files from Bach's \"The Musical Offering\"</p> In\u00a0[4]: Copied! <pre>directory = '/Users/chriskimmons/My_Projects/music_projects/musical_offering/'\n\nfiles = list(glob.glob(os.path.join(directory,'*.*')))\n\nfiles\n</pre> directory = '/Users/chriskimmons/My_Projects/music_projects/musical_offering/'  files = list(glob.glob(os.path.join(directory,'*.*')))  files Out[4]: <pre>['/Users/chriskimmons/My_Projects/music_projects/musical_offering/1079-02.mid',\n '/Users/chriskimmons/My_Projects/music_projects/musical_offering/IMSLP188980-WIMA.aee8-CanBachOff01.mid',\n '/Users/chriskimmons/My_Projects/music_projects/musical_offering/1079-03.mid',\n '/Users/chriskimmons/My_Projects/music_projects/musical_offering/1079-04.mid',\n '/Users/chriskimmons/My_Projects/music_projects/musical_offering/1079-05.mid',\n '/Users/chriskimmons/My_Projects/music_projects/musical_offering/1079-05.mid.mid']</pre> In\u00a0[5]: Copied! <pre>interpret(files[1])\n</pre> interpret(files[1]) <pre>pygame 2.4.0 (SDL 2.26.4, Python 3.8.8)\nHello from the pygame community. https://www.pygame.org/contribute.html\n</pre> In\u00a0[6]: Copied! <pre>bach = mu.corpus.getComposer('Bach')\n</pre> bach = mu.corpus.getComposer('Bach') In\u00a0[7]: Copied! <pre>interpret(bach[300])\n</pre> interpret(bach[300]) In\u00a0[8]: Copied! <pre>bach_stream = mu.converter.parse(bach[40])\n</pre> bach_stream = mu.converter.parse(bach[40]) In\u00a0[9]: Copied! <pre>bach_df = gab.convertScoreToDF(bach_stream)\n</pre> bach_df = gab.convertScoreToDF(bach_stream) In\u00a0[10]: Copied! <pre>bach_stream.show()\n</pre> bach_stream.show() In\u00a0[11]: Copied! <pre>graph_melodies(bach_df)\n</pre> graph_melodies(bach_df) In\u00a0[12]: Copied! <pre>play(bach_stream)\n</pre> play(bach_stream)"},{"location":"Melody_Graph/graphing_melodies/#functions","title":"Functions\u00b6","text":""},{"location":"Melody_Graph/graphing_melodies/#crab-canon","title":"Crab Canon\u00b6","text":"<p>This is an interpretation of Bach's Crab Canon, a famous musical puzzle that is played forwards and backwards, at the same time.  Look at the symmetry in the graph:</p>"},{"location":"Melody_Graph/graphing_melodies/#chorales","title":"Chorales\u00b6","text":"<p>Here we witch modes to Bach's Chorales.  Notice the independance of movement between the lines throughout the piece.  That style of writing became the hallmark of the classical style.</p>"}]}